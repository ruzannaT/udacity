{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Introduction\n",
    " \n",
    "The goal of this project is to provide a student a hands-on example of how to gather, clean and analyse real data. <a href=\"https://twitter.com/dog_rates\">\"WeRateDogs\"</a> Twitter account with over 4 million followers that rates people's dogs with a humorous comment about the dog was selected as data source. \n",
    "\n",
    "Project \"Wrangling and Analyse Data\" consist of following steps: \n",
    "* data gathering, \n",
    "* data assessment, \n",
    "* data cleaning, \n",
    "* data storing, \n",
    "* last but not least, data analysis and visualisation.\n",
    "\n",
    "Below you may find brief describtion of my wrangling efforts that will be covering gathering, assessment and cleaning parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 -  Data wrangling\n",
    "\n",
    "### Data gathering \n",
    "Data gathering process was split in 3 different sub-tasks: \n",
    "* first, read 'twitter-archive-enhanced' file to Data Frame; \n",
    "* second, read tweet image predictions tsv file by using \"Request\" library;\n",
    "* third, collect additional tweet data using Twitter API. Due to technical issues with obtaining developer twitter account, I leveraged provided by Udacity 'tweet-json.txt' file that containes additional data available through Twitter APIs and and then read json file line by line to a Data Frame.\n",
    "\n",
    "Each data source has been saved separately as following Data Frames: twitter_archive, image_predictions and tweet_data accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Assessment\n",
    "After saving each data Data Frame separately, I conducted visual and programmatic assessment to define tidiness and quality issues. During visual assessment I was randomly displaying and viewing subsets of Data Frames directly in jupyter notebook as well as using Excel or TextEdit programs. While doing programmatic assessment I was mainly using such methods as 'describe', 'info', 'sample' to defie issues related to data types. Data assessment resulted in a list of quality and tidiness issues that can be found below: \n",
    "\n",
    "#### Quality issues\n",
    "**a. twitter_archive:**\n",
    "\n",
    "    1. twitter_archive: in_reply_to_status_id, in_reply_to_user_id; retweeted_status_id; retweeted_status_user_id must be str instead of float since IDs.\n",
    "\n",
    "    2. timestamp and retweeted_status_timestamp values should be datetime type not object/string.\n",
    "\n",
    "    4. rating_denominator has incorrect values (> 10). --> check text message and use regex to retrieve digit caratters\n",
    "\n",
    "    5. Columns contain null objects that are non-null (None to NaN).\n",
    "    \n",
    "    6. 'Name' column contains invalid names (None or length is <= 3).\n",
    "    \n",
    "    7. We only want original ratings (no retweets) that have images.\n",
    "    \n",
    "    \n",
    "**b. image_predictions:**\n",
    "\n",
    "    1. Some tweet_ids refer to the same image URLs (66 duplicative jpg_url)\n",
    "    \n",
    "    2. Missing values in image_predictions: only 2075 entries instead of 2356\n",
    "    \n",
    "    3. p2-p3 predictions and conf. level will be not relevant for master file \n",
    "    \n",
    "    4. tweet_id in wrong format (must be str instead of int)\n",
    "    \n",
    "    \n",
    "**c. tweet_data:**\n",
    "\n",
    "    1. created_at contains date and time info --> should be ether:\n",
    "    - splitted and have propper type (datetime, date but not object/string \n",
    "    - or checked with creation info in other tables - delete if already part of other tables info\n",
    "\n",
    "#### Tidiness issues\n",
    "    1. Dog stages in 'twitter_archive' should be combined in one column with respective values: doggo, floofer, pupper, puppo\n",
    "\n",
    "    2. Join 'tweet_info' and 'image_predictions' to 'twitter_archive'\n",
    "\n",
    "    3. Uninformative columns (all except from 1 record of the place are null) in 'tweet_data': geo, coordinates, place, contributors\n",
    "\n",
    "    4. In 'tweet_data' id_str duplicates id values in int format\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Prior to starting data cleaning process, copies of Data Frames were created. \n",
    "* I kicked off the process with replacing all empty, None or 'None'(as a string) with NaN values. \n",
    "* Second step was to convert values to a correct data type: id values to string, dates from string type to timestamp type. \n",
    "* During third step, dog stages represented in different columns were melted in one column named 'dog_stage' with respective values: 'doggo', 'floofer', 'pupper', 'puppo'.\n",
    "* Forth step was to merge 3 data frames into one based on id. Then I had to clean up duplicated and uninformative columns.\n",
    "* Sixth step was to drop tweet records that do not have any pictures as per project requirement. \n",
    "* Seventh step was to ensure that rating values were retrieved correctly - it was re-retrieved by using regex. In addition new column called \"rating\" has been created: numerator/denominator. It was done due to a fact that some tweets had denominator values > 10 and in this way it is easier to compare tweets ratings by using new column \"rating\" values.\n",
    "* On second last step I was cleaning dogs' names by replacing values 'a' with NaN.\n",
    "* Lastly I retrieved gender information from tweet text where possible and converted it to a category data type.\n",
    "\n",
    "Cleaned data was saved as 'twitter_archive_master.csv' file as per requirements and going forward was used for further analysis and visualisation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
